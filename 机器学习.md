# 模型评估

选择与问题相匹配的评估方法，才能快速地发现模型选择或训练过程中出现的问题，迭代地对模型进行优化。

准确率当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。

Precision值和Recall值是既矛盾又统一的两个指标，为了提高Precision值，分类器需要尽量在“更有把握”时才把样本预测为正样本，但此时往往会因为过于保守而漏掉很多“没有把握”的正样本，导致Recall值降低。

- Precision：分类正确的正样本个数占分类器判定为正样本的样本个数的比例
- Recall：分类正确的正样本个数占真正的正样本个数的比例

F1 score是精准率和召回率的调和平均值：

$$
\mathrm{F} 1=\frac{2 \times \text { precision } \times \text { recall }}{\text { precision }+ \text { recall }}
$$

ROC曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR）

$$
\begin{aligned}
&F P R=\frac{F P}{N}\\
&T P R=\frac{T P}{P}
\end{aligned}
$$

- FPR：所有真实的负样本中有多少被模型预测成正类
- TPR：所有真实的正样本中有多少被模型预测成正类

AUC指的是ROC曲线下的面积大小。假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取⼀对（正、负）样本，正样本的score⼤于负样本的score的概率。

1. ROC曲线⽆视样本不平衡（为什么选⽤TPR以及FPR作为指标的原因）？

> TPR和FPR分别是基于实际表现1和0出发的，它们分别在实际的正样本和负样本中来观察相关概率问题。⽆论样本是否平衡，都不会被影响。总样本中，90%是正样本，10%是负样本。TPR只关注90%正样本中有多少是被真正覆盖的，⽽与那10%毫⽆关系，同理，FPR只关注10%负样本中有多少是被错误覆盖的，也与那90%毫⽆关 系，所以可以看出：如果我们从实际表现的各个结果⻆度出发，就可以避免样本不平衡的问题了，这也是为什么选⽤TPR和FPR 作为ROC/AUC的指标的原因。

2. 如果我们将模型在所有样本上的预估值都乘以2或者都减去0.01，则该模型模型的AUC会变化吗？

> AUC值等于随机选择的正样本值⾼于随机选择的负样本的概率。这两个操作并不会让AUC变化

3. 我们可以将数据随机切分成10份，选择其中9份作为训练集，1份作为测试集，实现10重交叉检验可以较好评估模型的效果吗？

> 应该是从正例中随机分成⼗份，分别与负例进⾏训练，类别不平衡问题不能随机切分

4. 如何计算AUC？

> 根据AUC的物理意义，我们计算正样本score⼤于负样本的score的概率。假设 $M$ 个正类样本，$N$ 个负类样本，流程如下：
> 1. 把所有样本按照score排序，依次⽤ $rank=0, 1, \cdots,n-1$ 表示他们，其中 $n=M+N$。最⼤score的样本对应的 $rank=n-1$。对相等score的样本，赋予相同的rank（⽆论在同类样本还是不同类的样本），具体操作就是再把所有这些score相等的样本对应的rank取平均
> 2. rank最⼤的正样本，有 $rank_{max}-(M-1)$ 个负样本⽐他score⼩
> 3. 次大正样本，有 $rank_{second}-(M-2)$ 个负样本⽐他score⼩
> 4. 最后我们得到正样本⼤于负样本的 概率，时间复杂度为 $O(N+M)$
> $$ AUC = \frac{\sum_{所有正样本 } rank - M(M-1) / 2}{M * N} $$

```python
def auc(ys, pros):
    M = sum(1 for y in ys if y == 1)
    N = sum(1 for y in ys if y == 0)
    res = [(p, y) for p, y in zip(pros, ys)]
    res.sort()
    rank = []
    r = 0
    while r < M + N:
        l = r
        while r < M + N and res[r][0] == res[l][0]:  # 概率 pro 相等，rank 去平均
            r += 1
        rank += [0.5 * (l + r - 1)] * (r - l)
    posRank = sum(rank[i] for i in range(M + N) if res[i][1] == 1)  # 正样本score⼤于负样本的个数
    return (posRank - 0.5 * M * (M - 1)) / (M * N)
```

# 正则化

模型所能提供的信息一般来源于两个方面，一是训练数据中蕴含的信息；二是在模型的形成过程中，人们提供的先验信息。先验信息可以作用在模型上，例如让模型采用特定的内在结构、条件假设或添加其他一些约束条件；先验信息也可以直接施加在数据集上，即根据特定的先验假设去调整、变换或扩展训练数据。

正则化（Regularization）主要目的是控制模型复杂度，减小过拟合。其数学表达形式为：

$$
\tilde{J}(w ; X, y)=J(w ; X, y)+\alpha \Omega(w)
$$

最常用的 $\Omega$ 函数有两种，即 $l_1$ 范数和 $l_2$ 范数，相应称之为 $l_1$ 正则化和 $l_2$ 正则化。

$$
\begin{aligned}
& l_1: \Omega(w)=\|w\|_1=\sum_i\left|w_i\right| \\
& l_2: \Omega(w)=\|w\|_2^2=\sum_i w_i^2
\end{aligned}
$$

## 约束条件的最优化

模型的复杂度可用VC维来衡量。通常情况下，模型VC维与系数 $w$ 的个数成线性关系。因此，为了限制模型的复杂度，很自然的思路是减少系数 $w$ 的个数，即限制 $w$ 中非零元素的个数。为此，我们可在原优化问题中加入一个约束条件：

$$
\begin{aligned}
& \min _w J(w ; X, y) \\
& \text { s.t. }\|w\|_0 \leq C
\end{aligned}
$$

$\|\cdot\|_0$ 范数表示向量中非零元素的个数。但由于该问题是一个NP问题。为了达到近似效果，我们不严格要求某些权重 $w$ 为0，而是要求权重 $w$ 应接近于0，即尽量小。从而可用 $l_1$ 、 $l_2$ 范数来近似 $l_0$ 范数，即：

$$
\begin{aligned}
\min _w J(w ; X, y) \quad \text { s.t. }\|w\|_1 \leq C \\
\min _w J(w ; X, y) \quad \text { s.t. }\|w\|_2^2 \leq C
\end{aligned}
$$

将上述带约束条件的最优化问题转换为不带约束项的优化问题，构造拉格朗日函数：

$$
\begin{aligned}
& L(w, \alpha)=J(w ; X, y)+\alpha\left(\|w\|_1-C\right) \\
& L(w, \alpha)=J(w ; X, y)+\alpha\left(\|w\|_2^2-C\right)
\end{aligned}
$$

其中 $\alpha>0$，我们假设 $\alpha$ 的最优解为 $\alpha^*$，则对拉格朗日函数求最小化等价于:

$$
\begin{array}{r}
\min _w J(w ; X, y)+\alpha^*\|w\|_1 \\
\min _w J(w ; X, y)+\alpha^*\|w\|_2^2
\end{array}
$$

上式与 $\min _w \tilde{J}(w ; X, y)$ 等价，我们得到对 $l_1$ 、 $l_2$ 正则化的第一种理解：

- $l_1$ 正则化：在原优化目标函数中增加约束条件 $\|w\|_1 \leq C$ 
- $l_2$ 正则化：在原优化目标函数中增加约束条件 $\|w\|_2^2 \leq C$

## 最大后验概率估计

在最大似然估计中，是假设权重 $w$ 是未知的参数，从而求得对数似然函数：

$$
l(w)=\log P(y \mid X ; w)=\log \left(\prod_i P\left(y^i \mid x^i ; w\right)\right)
$$

在最大后验概率估计中，则将权重 $w$ 看作随机变量，也具有某种分布，从而有：

$$
\begin{aligned}
P(w \mid X, y)&=\frac{P(w, X, y)}{P(X, y)}=\frac{P(X, y \mid w) P(w)}{P(X, y)} \propto P(y \mid X, w) P(w) \\
l(w)&=\log P(y \mid X, w) P(w)=\log P(y \mid X, w)+\log P(w)
\end{aligned}
$$

$P(w)$ 是对权重系数 $w$ 的概率分布的先验假设，在收集到训练样本 $\{X, y\}$ 后, 则可根据 $w$ 在 $\{X, y\}$ 下的后验概率对 $w$ 进行修正，从而做出对 $w$ 更好地估计。

若假设 $w_j$ 的先验分布为 0 均值的高斯分布，即 $w_j \sim N\left(0, \sigma^2\right)$，则有:

$$
\log P(w)=\log \prod_j P\left(w_j\right)=\log \prod_j\left[\frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{\left(w_j\right)^2}{2 \sigma^2}}\right]=-\frac{1}{2 \sigma^2} \sum_j w_j^2+C^{\prime}
$$

若假设 $w_j$ 服从均值为 0 、参数为 $a$ 的拉普拉斯分布，即：$P\left(w_j\right)=\frac{1}{\sqrt{2 a}} e^{\frac{-\left|w_j\right|}{a}}$，则有：

$$
\log P(w)=\log \prod_j P\left(w_j\right)=\log \prod_j \frac{1}{\sqrt{2 a}} e^{\frac{-\left|w_j\right|}{a}}=-\frac{1}{a} \sum_j\left|w_j\right|+C^{\prime}
$$

我们得到对于 $l_1$ 、 $l_2$ 正则化的第二种理解：

- $l_1$ 正则化：对参数 $w$ 引入了拉普拉斯先验，拉普拉斯分布在极值点（0点）处是一个尖峰。即先验分布中认为参数 $w$ 取值为0的可能性要更高
- $l_2$ 正则化：对参数 $w$ 引入了高斯先验，高斯分布在极值点（0点）处是平滑的。即先验分布中认为参数 $w$ 在极值点附近取不同值的可能性是接近的

## 直观理解

考虑带约束条件的优化解释，对 $l_2$ 正则化为：

$$
\begin{aligned}
& \min _w J(w ; X, y) \\
& \text { s.t. }\|w\|_2 \leq C
\end{aligned}
$$

<img src="assets/v2-7431d8a79deec5d0ab3193b6a3611b95_720w.webp" alt="img" style="zoom:50%;" />

椭圆为原目标函数 $J(w)$ 的一条等高线，圆为半径 $\sqrt{C}$ 的 $l_2$ 范数球。由于约束条件的限制，因而在使用梯度下降法更新 $w$ 时，只能朝 $\nabla J(w)$ 在 范数球上 $w$ 处的切线方向更新，即图中绿色箭头的方向。当 $\nabla J(w)$ 与范数球上 $w$ 处的法线平行时，此时 $\nabla J(w)$ 在切线方向的分量为 0，$w$ 将无法继续移动，从而达到最优解 $w^*$ (图中红色点所 示)。

对于 $l_1$ 正则化：

$$
\begin{aligned}
& \min _w J(w ; X, y) \\
& \text { s.t. }\|w\|_1 \leq C
\end{aligned}
$$

<img src="assets/image-20230625044412808.png" alt="image-20230625044412808" style="zoom:50%;" />

其主要差别在于 $l_1 、 l_2$ 范数球的形状差异。由于此时每条边界上 $w$ 的切线和法线方向保持不变，在图中 $w$ 将一直朝着 $\nabla J(w)$ 在切线方向的分量沿着边界向左上移动。当 $w$ 跨过顶点到达 $w^{\prime}$ 时，$\nabla J(w)$ 在切线方向的分量变为右上方， $w$ 将朝右上方移动。最终，$w$ 将稳定在顶点处，达到最优解 $w^*$ 。此时，$w_1=0$ 这也就是采用 $l_1$ 范数会使 $w$ 产生稀疏性的原因。

# Normalization

数据和特征往往决定了结果的上限，而模型、算法的选择及优化则是在逐步接近这个上限。特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。

为了消除数据特征之间的量纲影响，使得不同指标之间具有可比性。数据归一化并不是万能的，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。但对于决策树模型则并不适用。 

## Batch Normalization

对于一个拥有 $d$ 维的输入 $x$ ，我们将对它的每一个维度进行标准化处理。假设我们输入的 $x$ 是 RGB 三通道的彩色图像，那么这里的 $d$ 就是输入图像的 channels 即 $d=3$ ，$x=\left(x^{(1)}, x^{(2)}, x^{(3)}\right)$， 其中 $x^{(1)}$ 就代表我们的 R 通道所对应的特征矩阵，依此类推。标准化处理也就是分别对我们的 R 通道，G 通道，B 通道进行处理。

$$
\begin{aligned}
\text { Input: } & \text { Values of } x \text { over a mini-batch: } \mathcal{B}=\left\{x_{1 \ldots m}\right\}; \text { Parameters to be learned: } \gamma, \beta \\
\text { Output: } & \left\{y_i=\mathrm{BN}_{\gamma, \beta}\left(x_i\right)\right\} \\
\mu_{\mathcal{B}} & \leftarrow \frac{1}{m} \sum_{i=1}^m x_i \quad / / \text { mini-batch mean } \\
\sigma_{\mathcal{B}}^2 & \leftarrow \frac{1}{m} \sum_{i=1}^m\left(x_i-\mu_{\mathcal{B}}\right)^2 \quad / / \text { mini-batch variance } \\
\widehat{x}_i & \leftarrow \frac{x_i-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}} \quad / / \text { normalize } \\
y_i & \leftarrow \gamma \widehat{x}_i+\beta \equiv \mathrm{BN}_{\gamma, \beta}\left(x_i\right) \quad / / \text { scale and shift }
\end{aligned}
$$

- $\mu_{\mathrm{B}}$：代表着每个维度 (channel) 的均值，$\mu_{\mathrm{B}}$ 向量的每一个元素代表着一个维度 (channel) 的均值
- $\sigma_B^2$：代表着每个维度 (channel) 的方差，$\sigma_B^2$ 向量的每一个元素代表着一个维度 (channel) 的方差

根据从 $\mu_{\mathrm{B}}$ 和 $\sigma_B^2$ 计算标准化处理后得到的值。

下图给出一个batch size 为2 (两张图片) 的Batch Normalization的计算过程的示例：

<img src="assets/image-20230527230817101.png" alt="image-20230527230817101" style="zoom:75%;" />

## Layer Normalization

BN 要求计算统计量的时候必须在同一个 Mini-Batch 内的实例之间进行统计，Batch 内实例之间存在相互依赖和影响的关系。

如何从根本上解决这些问题？一个自然的想法是：把对 Batch 的依赖去掉，转换统计集合范围。在统计均值方差的时候，不依赖 Batch 内数据，只用当前处理的单个训练数据来获得均值方差的统计量，这样因为不再依赖 Batch 内其它训练数据，那么就不存在因为 Batch 约束导致的问题。

但是这个指导思路尽管会解决 BN 带来的问题，又会引发新的问题：此时统计范围必须局限在一个训练实例内。所以核心问题是对于单个训练实例，统计范围怎么算？

这就是 LN 的基本思想：我们完全可以直接用同层隐层神经元的响应值作为集合 $S$ 来求均值和方差。

- MLP：同一隐层自己包含了若干神经元

<img src="assets/640.png" alt="img" style="zoom:50%;" />

- CNN：同一个卷积层包含 $k$ 个输出通道，每个通道包含 $m*n$ 个神经元，整个通道包含了 $k*m*n$ 个神经元

<img src="assets/640-20230528165032872.png" alt="img" style="zoom:50%;" />

- RNN：每个时间步的隐层也包含了若干神经元

<img src="assets/640-20230528165048076.png" alt="img" style="zoom:50%;" />

LN 目前看好像也只适合应用在 RNN 这种动态网络，在 CNN 等环境下效果是不如 BN 或者 GN 等模型的。值得注意的是，在每个时间步，$\gamma$ 和 $\beta$ 都是共享的，它们的维度都是和隐层神经元个数一样。

## Instance Normalization

LN 抛开对 Mini-Batch 的依赖，为了能够统计均值方差，把同层内所有神经元的响应值作为统计范围，那么我们能否进一步将统计范围缩小？

对于 CNN 明显是可以的，因为同一个卷积层内每个卷积核会产生一个输出通道，而每个输出通道是一个二维平面，也包含多个激活神经元，自然可以进一步把统计范围缩小到单个卷积核对应的输出通道内部。

<img src="assets/640-20230528171513212.png" alt="img" style="zoom:50%;" />

对于 RNN 或者 MLP ，如果在同一个隐层类似 CNN 这样缩小范围，那么就只剩下单独一个神经元，没有形成集合 $S$，所以 RNN 和 MLP 是无法进行 IN 操作的。

## Group Normalization

从上面的 LN 和 IN 可以看出，这是两种极端情况，Layer Normalization 是将同层所有神经元作为统计范围，而 IN 则是 CNN 中将同一卷积层中每个卷积核对应的输出通道单独作为自己的统计范围。通道分组是 CNN 常用的模型优化技巧， GN 的核心思想：对 CNN 中某一层卷积层的输出或者输入通道进行分组，在分组范围内进行统计。

<img src="assets/640-20230528171800883.png" alt="img" style="zoom:50%;" />

理论上 MLP 和 RNN 也可以引入这种模式，不过MLP 和 RNN 这么做的话，分组内包含神经元太少，估计缺乏统计有效性，猜测效果不会太好。

## Normalization 为何有效

BN 能加快神经网络收敛速度，不再依赖精细的参数初始化过程，可以使用较大的学习率等很多好处。但是所讲的这些好处仅仅是引用 BN 带来的结果，那么更深层次的问题是：为什么 BN 能够给深度学习带来如此多的优点呢？

### 内部协变量偏移(Internal Covariate Shift)

<img src="assets/image-20230528175102147.png" alt="image-20230528175102147" style="zoom:50%;" />

当前一层 $  A \rightarrow A^{\prime} $ 的时候，它的 output：$  a \rightarrow a^{\prime} $ ，我们计算 $ \frac{\partial l}{\partial B}$ 的时候依赖的是 $a$ ，所以梯度方向也许更适合用在 $ a  $ 上，但不适合用在 $ a^{\prime} $。BN 让 $a$ 和 $ a^{\prime} $ 的分布比较接近，也许对训练有帮助。

原始的 BN 论文给出的解释是 BN 可以解决神经网络训练过程中的 ICS（Internal Covariate Shift）问题。但是能够解决 ICS 问题其实并不是 BN为 何有效背后真正的原因。ICS 问题在较深的网络中确实是普遍存在的，但是这并非导致深层网络难以训练的根本原因。另外，BN 其实也没有解决了 ICS 问题。实验一方面证明：即使是应用了 BN ，网络隐层中的输出仍然存在严重的 ICS 问题；另一方面也证明了：在 BN 层输出后人工加入噪音模拟 ICS 现象，并不妨碍 BN 的优秀表现。这两方面的证据侧面说明了 BN 和 ICS 问题并没什么关系。

### 损失曲面(Loss Surface)

在深度网络叠加大量非线性函数方式来解决非凸复杂问题时，损失曲面（Loss Surface）形态异常复杂，是由平坦的大量充满鞍点的曲面构成，训练过程就是利用 SGD 在这个复杂平面上一步一步游走，期望找到全局最小值。在如此复杂曲面上寻找全局最小值而不是落入局部最小值或者被困在鞍点动弹不得，可想而知难度有多高。

Normalization 通过对激活值进行参数重整，对复杂的损失曲面有很好的平滑作用，与其对应的重整后梯度也变得更平滑，更有利于SGD寻优找到问题好的解决方案，因此缓解梯度消失或梯度爆炸问题。

## 结束语

所有 Normalization 都采取了类似的步骤和过程，将神经元的激活值重整为均值为 0 方差为 1 的新数值，最大的不同在于计算统计量的神经元集合 $S$ 的划分方法上。

在CV中常常使用BN，它是在NHW维度进行了归一化，而Channel维度的信息原封不动，因为可以认为在CV应用场景中，数据在不同channel中的信息很重要，如果对其进行归一化将会损失不同 channel的差异信息。

而 NLP 中不同 batch 样本的信息关联性不大，而且由于不同的句子长度不同，强行归一化会损失不同样本间的差异信息，所以就没在batch 维度进行归一化，而是选择LN：只考虑的句子内部维度的归一化。

以 BERT 每一层 bert_tensor 的维度：[batch_size, seq_len, hidden_size] 为例：

- BN 是在 batch_size 维做 Norm，则：

```python
for i in range(seq_len):
    for j in range(hidden_size):
        Norm([bert_tensor[k][i][j] for k in range(batch_size)])
```

- LN是在 hidden_size 维做 Norm，则：

```python
for i in range(batch_size):
    for j in range(seq_len):
        Norm([bert_tensor[i][j][k] for k in range(hidden_size)])
```

总结一下：选择什么样的归一化方式，取决于你关注数据的哪部分信息。如果某个维度信息的差异性很重要，需要被拟合，那就别在那个维度进行归一化。

# 梯度下降法

## 随机梯度下降

$$
\theta_{t+1}=\theta_t-\eta g_t
$$

## 动量（Momentum）

$$
\begin{aligned}
v_t &=\gamma v_{t-1}+\eta g_t\\
\theta_{t+1}&=\theta_t-v_t 
\end{aligned}
$$

惯性就体现在对前一次步伐信息的重利用上，解决随机梯度下降法山谷震荡和鞍点停滞的问题。当前梯度就好比当前时刻受力产生的加速度，前一次步伐好比前一时刻的速度，当前步伐好比当前时刻的速度。为了计算当前时刻的速度，应当考虑前一时刻速度和当前加速度共同作用的结果。

## AdaGrad

希望更新频率低的参数可以拥有较大的更新步幅，而更新频率高的参数的步幅可以减小。AdaGrad方法采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏：

$$
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{\sum_{k=0}^t g_{k, i}^2+\epsilon}} g_{t, i}
$$

## Adam

Adam方法将惯性保持和环境感知这两个优点集于一身。一方面，Adam记录梯度的一阶矩，即过往梯度与当前梯度的平均，这体现了惯性保持；另一方面，Adam还记录梯度的二阶矩，即过往梯度平方与当前梯度平方的平均，这类似AdaGrad方法，体现了环境感知能力，为不同参数产生自适应的学习速率：

$$
\begin{aligned}
&m_t=\beta_1 m_{t-1}+\left(1-\beta_1\right) g_t \\
&v_t=\beta_2 v_{t-1}+\left(1-\beta_2\right) g_t^2 
\end{aligned}
$$

Adam的更新公式为：

$$
\begin{aligned}
\theta_{t+1}&=\theta_t-\frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t+\epsilon}} \\
\hat{m}_t&=\frac{m_t}{1-\beta_1^t} \\
\hat{v}_t&=\frac{v_t}{1-\beta_2^t}
\end{aligned}
$$

# 文本表示模型

## 词袋模型

将每篇文章看成一袋子词，并忽略每个词出现的顺序。常用 $TF-IDF$ 来评估某个词对一个语料库某篇文章重要程度：

$$
T F-I D F=T F \times I D F=\frac{N_{d, t}}{N_d} \times \log \left(\frac{M}{M_t+1}\right)
$$

- 词频 $T F=\frac{N_{d, t}}{N_d}$ ：某个词 $t$ 在文章 $d$ 中出现的总次数为 $N_{d, t}$，文章 $d$ 的总词数为 $N_d$
- 逆文档频率 $I D F=\log \left(\frac{M}{M_t+1}\right)$：语料库共有 $M$ 篇文章，其中包含词 $t$ 的文章数为 $M_t$

$TF-IDF$ 的核心思想：一个词在某篇文章中出现的次数越多，但在所有其他文章中出现的频率越低，则该词与该文章的相关性越强、越能代表该文章的主题、越能使文章具有区分度。

## Word2Vec

将文章进行词级别的划分有时候并不是一种好的做法，将连续出现的 n 个词（n≤N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成N-gram模型。

词嵌入核心思想是将每个词都映射成低维空间上的一个稠密向量，Word2Vec是目前最常用的词嵌入模型之一。Word2Vec的基本思想：一个词的语义可以由它的上下文确定；如果两个词的上下文相似，那么这两个词的语义就相似。Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，分别CBOW和Skip-gram。

CBOW的目标是根据上下文出现的词语来预测当前词的生成概率；而Skip-gram是根据当前词来预测上下文中各词的生成概率。

SkipGram 举例如下：

$$
\begin{aligned}
J(\theta) & =\prod_{w } \prod_{c } P(c \mid \omega ; \theta) \\
& = \sum_{w } \sum_{c } \log P(c \mid \omega ; \theta) \\
& = \sum_{w } \sum_{c } \log \frac{e^{u_c^T v_w}}{\sum _{c' \in V} e^{u_{c'}^T v_w}} \\
& = \sum_w \sum_c\left(u_c^T v_w-\log \sum_{c^{\prime} \in V} e^{u_{c'}^T v_w}\right) 
\end{aligned}
$$

- $w \in text$ 是中心词
- $c \in context(w)$ 是 $w$ 的上下文词

分别定义input向量 $V$ 和 output向量 $U$。每个词有两种角色，分别为中心词和预测词。定义两种词向量可以把这两种角色分开，角色的作用不同语义不同。

由于Softmax存在归一化项的缘故，迭代需要对词汇表中的所有单词进行遍历，每个训练样本需要更新所有的权重，使得每次迭代过程非常缓慢，由此产生了Hierarchical Softmax和Negative Sampling两种改进方法。

Negative Sampling 将多分类问题转换为对每两个词构造二分类，一个训练样本仅更新部分权重。

$$
\begin{aligned}
J(\theta) & = \prod_{(w,c)} P\left(D=1 \mid u_c , v_w\right) \prod_{\left(w, c^{\prime}\right)} P\left(D=0 \mid u_c' , v_w\right) \\
& = \prod_{(\omega, c)} \sigma \left(u_c^T  v_w\right) \prod_{(w,c')} \left(1 - \sigma \left(u_{c'}^T v_w\right) \right) \\
& = \prod_{(\omega, c)} \sigma \left(u_c^T  v_w\right) \prod_{(w,c')}  \sigma \left(-u_{c'}^T v_w\right)  \\
& = \sum_{(w, c)} \log \sigma \left(u_c^T  v_w\right)+\sum_{(w,c')} \log \sigma \left(-u_{c'}^T v_w\right) \\

\frac{\partial J(\theta)}{\partial u_c} &= \frac{1}{\sigma\left(u_c^T v_w\right)}  \sigma\left(u_c^T v_w\right)\left[1-\sigma\left(u_c^T v_w\right)\right]  v_w \\
&= \left[1-\sigma \left(u_c^T v_w\right)\right] v_w \\

\frac{\partial J(\theta)}{\partial v_w} &= \frac{1}{\sigma\left(u_c^T v_w\right)}  \sigma\left(u_c^T v_w\right)\left[1-\sigma\left(u_c^T v_w\right)\right]  u_c + \sum_{c' \in N(w)}\left[\frac{1}{\sigma\left(-u_{c'}^T v_w\right)} \sigma\left(-u_{c'}^T v_w\right)\left[1-\sigma\left(-u_{c'}^T v_w\right)\right] \left(-u_{c'}\right)\right] \\
&= \left[1-\sigma\left(u_c^T v_w\right)\right]  u_c+\sum_{c' \in N(w)}\left[\sigma\left(-u_{c'}^T v_w\right)-1\right] u_c

\end{aligned}
$$

Word2Vec的最终目的不是要把这个语言模型到练的很完美，而是只关心模型训练之后的副产物（模型的参数），并将其作为 $x$ 的向量表示（词向量）。当模型训练完毕后，最后得到的其实是神经网络的权重。在输入层到隐含层的权重里，只有对应为 $1$ 的权重被激活，而每个词语的 one-hot 里面 $1$ 的位置是不同且唯一的，从而用这些权重组成的向量可以用来唯一表示 $x$ 。
SkipGram 每个词作为中心间时都要使用周围词进行预测一次。当数据量较少或者生僻词较多时，SkipGram相对更加准确，时间复杂度为 $O(KV)$；CBOW进行调整时，梯度的值会平均分配到该词上，生僻词没有受到专门训练，时间复杂度为 $O(V)$。
SkipGram相当于1个学生（中心词）VS K个老师（周围词），每个周围词都会对中心词进行训练是私教；而CBOW是K个学生（周围词）VS 1 个老师（中心词），中心词是一视同仁的教给周围词一样的知识是公共课。

# 熵和KL散度

熵是一个随机事件所包含信息量的期望；是对随机事件中的随机变量进行编码所需的最小字节数：

$$
H(p)=-\sum_x p(x) \ln p(x)
$$

- 越不可能发生的事件信息量越大，而确定事件的信息量就很低。
- 那些接近确定性的分布（输出几乎可以确定）具有较低的熵。
- 那些接近均匀分布的概率分布具有较高的熵。

KL散度，也称为相对熵（Relative Entropy），是用来衡量两个概率分布之间的差异的一种度量方式。它衡量的是当用一个分布 $Q$ 来拟合真实分布 $P$ 时所需要的额外信息的平均量。

$$
D_{K L}(P \| Q)=H(P, Q)-H(P)=-\sum_x P(x) \log \frac{Q(x)}{P(x)}
$$

交叉熵指的是当用 $Q$ 来表示 $P$ 所需要的平均编码长度。$KL$ 散度的意义是用 $Q$ 来表示 $P$ 额外需要的编码长度，即信息损失是多少。在机器学习评价两个分布之间的差异时，由于分布 $P$ 会是给定 的, 所以此时 $KL$ 散度和交叉熵的作用其实是一样的，而且因为交叉熵少算一项，更加简单，所以选择交叉熵会更好。

比如含有4个字母 $(A,B,C,D)$ 的数据集中，真实分布 $P=(1/2, 1/2, 0, 0)$，即 $A$ 和 $B$ 出现的概率均为 $1/2$，$C$ 和 $D$ 出现的概率都为 $0$。计算 $H(P)=1$，即只需要1位编码即可识别 $A$ 和 $B$。如果使用分布 $Q=(1/4, 1/4, 1/4, 1/4)$ 来编码，则得到 $H(P,Q)=2$，即需要 $2$ 位编码来识别 $A$ 和 $B$（尽管 $C$ 和 $D$并不会出现）。

# 决策树

决策树是一种自上而下，对样本数据进行树形分类的过程。结点分为内部结点和叶结点，其中每个内部结点表示一个特征或属性，叶结点表示类别。

## ID3

对于样本集合D，类别数为K，数据集D的经验熵表示为：

$$
H(D)=-\sum_{k=1}^K \frac{\left|C_k\right|}{|D|} \log _2 \frac{\left|C_k\right|}{|D|}
$$

某个特征A对于数据集D的经验条件熵 $H(D|A)$ 为：

$$
H(D \mid A)=\sum_{i=1}^n \frac{\left|D_i\right|}{|D|} H\left(D_i\right)=\sum_{i=1}^n \frac{\left|D_i\right|}{|D|}\left(-\sum_{k=1}^k \frac{\left|D_{i k}\right|}{\left|D_i\right|} \log _2 \frac{\left|D_{i k}\right|}{\left|D_i\right|}\right)
$$


于是信息增益 $g(D,A)$ 可以表示为二者之差，可得

$$
g(D, A)=H(D)-H(D \mid A)
$$

ID3是采用信息增益作为评价标准，会倾向于取值较多的特征。因为，信息增益反映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性更高，也就是条件熵越小，信息增益越大。ID3缺少对取值较多特征的惩罚。

## C4.5

特征A对于数据集D的信息增益比定义为：

$$
\begin{aligned}
g_R(D, A)&=\frac{g(D, A)}{H_A(D)} \\
H_A(D)&=-\sum_{i=1}^n \frac{\left|D_i\right|}{|D|} \log _2 \frac{\left|D_i\right|}{|D|}
\end{aligned}
$$

C4.5实际上是对ID3进行优化，通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，避免ID3出现过拟合的特性，提升决策树的泛化能力。ID3只能处理离散型变量，C4.5处理连续型变量时，通过对数据排序之后找到类别不同的分割线作为切分点，从而将连续型变量转换多个取值区间的离散型变量。

## CART

基尼指数（Gini）描述的是数据的纯度，与信息熵含义类似：

$$
\operatorname{Gini}(D)=1-\sum_{k=1}^n\left(\frac{\left|C_k\right|}{|D|}\right)^2
$$

> $\begin{aligned} f(x) & =\log x \approx f(1)+f^{\prime}(1)(x-1)=x-1 \\ H(p) & =-\sum_x p(x) \log p(x) \\ & =-\sum_x p(x)[p(x)-1] \\ & =\sum_x p(x)-\sum_x p^2(x) \\ & =1-\sum_x p^2(x)\end{aligned}$

CART在每一次迭代中选择基尼指数最小的特征及其对应的切分点进行分类。但与ID3、C4.5不同的是，CART是一颗二叉树，采用二元切割法，每一步将数据按特征A的取值切成两份，分别进入左右子树。特征A的Gini指数定义为：

$$
\operatorname{Gini}(D \mid A)=\sum_{i=1}^n \frac{\left|D_i\right|}{|D|} \operatorname{Gini}\left(D_i\right)
$$

ID3和C4.5只能用于分类任务，而CART不仅可以用于分类，也可以应用于回归任务（回归树使用最小平方误差准则）。主要是特征分裂的评价标准不一样，CART树有两种：Variance和Gini系数。而ID3和C4.5的评价基础都是信息熵，信息熵和Gini系数是针对分类任务的指标，Variance是针对连续值的指标。

## 剪枝

预剪枝，即在生成决策树的过程中提前停止树的增长。而后剪枝，是在已生成的过拟合决策树上进行剪枝。

预剪枝的核心思想是在树中结点进行扩展之前，先计算当前的划分是否能带来模型泛化能力的提升，如果不能，则不再继续生长子树。此时可能存在不同类别的样本同时存于结点中，按照多数投票的原则判断该结点所属类别。

后剪枝的核心思想是让算法生成一棵完全生长的决策树，然后从最底层向上计算是否剪枝。剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样按照多数投票的原则进行判断。

代价复杂剪枝主要包含以下两个步骤：

1. 从完整决策树 $T_0$ 开始，生成一个子树序列 $\left\{T_0, T_1, T_2, \cdots, T_n\right\}$，其中 $T_{i+1}$ 由 $T_i$ 生成，$T_n$ 为树的根结点
2. 在子树序列中，根据真实误差选择最佳的决策树

从 $T_0$ 开始，裁剪 $T_i$ 中关于训练数据集合误差增加最小的分支以得 到 $T_{i+1}$ 。具体地，当一棵树 $T$ 在结点 $t$ 处剪枝时，它的误差增加可以用 $R(t)-R\left(T_t\right)$ 表示，其中 $R(t)$ 表示进行剪枝之后的该结点误差，$R\left(T_t\right)$ 表示末进行剪枝时子树 $T_t$ 的误 差。考虑到树的复杂性因素，我们用 $\left|L\left(T_t\right)\right|$ 表示子树 $T_t$ 的叶子结点个数，则树在结 点 $t$ 处剪枝后的误差增加率为：

$$
\alpha=\frac{R(t)-R\left(T_t\right)}{ \left|L\left(T_t\right)\right| - 1}
$$

在得到 $T_i$ 后，我们每步选择 $\alpha$ 最小的结点进行相应剪枝。

> 剪枝前：$loss(T_t) = R(T_t) + \alpha \left|L\left(T_t\right)\right|$
> 剪枝后：$loss(t) = R(t) + \alpha$
> 当 $\alpha = 0$ 时，未剪枝的损失最小；当 $\alpha = \infty$ 时，单结点的决策树损失最小
> $loss\left(T_t\right)=loss(t) \Rightarrow \alpha=\frac{R(t)-R\left(T_t\right)}{\left|T_t\right|-1}$
> 此时，$T_t$ 和 $t$ 有相同的损失值，但 $t$ 的结点更少，因此 $t$ 比 $T_t$ 更可取，对 $T_t$ 进行剪枝

# K-Means

K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，所以先要做归一化处理。同时，离群点或者少量的噪声数据会导致中心偏移，因此使用K均值聚类算法之前通常需要对数据做预处理。

通过迭代方式寻找K个簇的一种划分方案，使得聚类结果对应的代价函数最小：
$$
\begin{aligned}
J&=\sum_i \sum_k \gamma_{i k}\left\|x_i-u_k\right\|^2 \\
\gamma_{i k}&= \begin{cases}1 \quad k=\underset{j}{argmin} \left\|x_i-u_j\right\|^2 \\
0 \quad other \end{cases}
\end{aligned}
$$

算法的具体步骤描述如下：

1. 数据预处理，如归一化、离群点处理等
2. 随机选取 $K$ 个簇中心, 记为 $\mu_1^{(0)}, \mu_2^{(0)}, \cdots, \mu_K^{(0)}$
3. 重复下面过程直到 $J$ 收敛：
	- 对于每一个样本 $x_i$，将其分配到距离最近的簇
	- 对于每一个类簇 $k$，重新计算该类簇的中心

原始K均值算法最开始随机选取数据集中 $K$ 个点作为聚类中心，而K-means++在选取第n+1个聚类中心时，距离前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。

# 高斯混合模型GMM

高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来的。在该假设下，每个单独的分模型都是标准高斯模型，其均值 $\mu_i$ 和方差 $\Sigma_i$ 是待估计的参数。此外，每个分模型都还有一个参数 $\pi_i$，可以理解为权重或生成数据的概率。

$$
p(x)=\sum_{i=1}^K \pi_i N\left(x \mid \mu_i, \Sigma_i\right)
$$

高斯混合模型与K均值算法的相同点是，它们都是可用于聚类的算法；都需要指定K值；都是使用EM算法来求解；都往往只能收敛于局部最优。而它相比于K均值算法的优点是，可以给出一个样本属于某类的概率是多少。

EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。当概率模型中含有无法被观测的隐含变量 $z^{(i)}$ 时，无法直接通过最大似然估计求解参数。假设 $\mathrm{z}^{(i)}$ 对应的分布为 $Q\left(z^{(i)}\right)$，利用Jensen不等式，可以得到：

$$
\begin{aligned}
J(\theta)&=\sum_{i} \log P\left(x^{(i)} \mid \theta\right) \\
&=\sum_{i} \log \sum_{z^{(i)}} P\left(x^{(i)}, z^{(i)} \mid \theta\right) \\
&=\sum_{i} \log \sum_{z^{(i)}} Q\left(z^{(i)}\right) \frac{P\left(x^{(i)}, z^{(i)} \mid \theta\right)}{Q\left(z^{(i)}\right)} \\
&\geqslant \sum_{i} \sum_{z^{(i)}} Q\left(z^{(i)}\right) \log \frac{P\left(x^{(i)}, z^{(i)} \mid \theta\right)}{Q\left(z^{(i)}\right)} \\
\end{aligned}
$$

要使上式中的等号成立，需要满足 $\frac{P\left(x^{(i)}, z^{(i)} \mid \theta\right)}{Q\left(z^{(i)}\right)}=c$，其中 $c$ 为常数。此时 $Q\left(z^{(i)}\right)=\frac{P\left(x^{(i)}, z^{(i)} \mid \theta\right)}{\sum_{z^{(i)}} P\left(x^{(i)}, z^{(i)} \mid \theta\right)}=P\left(z^{(i)} \mid x^{(i)}, \theta\right)$ 。

- E步骤：计算隐变量的期望：

$$
Q\left(z^{(i)}\right)=P\left(z^{(i)} \mid x^{(i)}, \theta\right)
$$

- M步骤：

$$
\begin{aligned}
\theta&=\underset{\theta}{argmax} \sum_{i} \sum_{z^{(i)}} Q\left(z^{(i)}\right) \log \frac{P\left(x^{(i)}, z^{(i)} \mid \theta\right)}{Q\left(z^{(i)}\right)} \\
&=\underset{\theta}{argmax} \sum_{i} \sum_{z^{(i)}} Q\left(z^{(i)}\right) \log \frac{P\left(x^{(i)}, z^{(i)} \mid \theta\right)}{Q\left(z^{(i)}\right)} \\
&=\underset{\theta}{argmax} \sum_{i} \sum_{z^{(i)}} Q\left(z^{(i)}\right) \log P\left(x^{(i)}, z^{(i)} \mid \theta\right) \\
&=\underset{\theta}{argmax} \sum_{i} \sum_{z^{(i)}} Q\left(z^{(i)}\right) \log \left[P( z^{(i)}) P\left(x^{(i)}\mid  z^{(i)}, \theta\right)\right ]
\end{aligned}
$$

对于每个观测 $x$，都有一个隐藏变量 $z$。在这种情况下，关于 $z$ 的取值，只能来源于后验概率的推算 $P(z \mid x)$，即E步；接下来，我们用近似完整的数据通过最大似然修正参数估计，也就是M步。

具体到高斯混合模型的求解，EM算法的迭代过程如下：

- E步骤：

$$
Q\left(z^i=k\right)=\frac{P(z^i=k) P\left(x^i \mid u_k , \Sigma_k\right)}{\sum_k P(z^i=k)  P\left(x^i \mid u_k ,  \Sigma_k\right)}=\gamma_{i k}
$$

- M步骤：

$$
\begin{aligned}
\theta&=\underset{\theta}{argmax} \sum_i \sum_k \gamma_{i k}  \log \left[P(z^i=k) P\left(x^i \mid u_k, \Sigma_k\right)\right] \\
\pi_k&=\frac{\sum_i \gamma_{i k}}{N} \\
u_k&=\frac{1}{N} \sum_i \gamma_{i k}  x_i \\
\Sigma_k&=\frac{1}{N} \sum_i \gamma_{ik} \left(x_i-u_k\right)\left(x_i-u_k\right)^T
\end{aligned}
$$

# 概率图模型

概率判别模型求的是 $P(y \mid x)$，将 label 根据提供的特征学习，最后画出了一个比较明显的边界；概率生成模型求的是联合概率分布 $P(x,y)$，对于新的数据，通过贝叶斯公式化简 $P(y \mid x) \propto P(x, y)$，从而得到 $P(y \mid x)$ 。生成模型关注结果是如何产生的，需要非常多的数据量来保证采样到了数据本来的面目，所以速度慢。

## Naive Bayes

$$
\begin{aligned}
P\left(x_i, y_i\right) & =P\left(y_i\right)  P\left(x_i \mid y_i\right) =\prod_k P\left(y_i=k\right)^{N(y_i, k)}  \prod_n P\left(w_n \mid y_i=k\right)^{N\left(x_i , k\right)} \\
\log P(x , y) & =\sum_i \log P\left(x_i, y_i\right) =\sum_i\left[\sum_k N\left(y_i, k\right)  \log \Phi_k+\sum_n N\left(x_i, k\right) \log \gamma_{k n}\right] \\
\frac{\partial l}{\partial \Phi_k}&=\sum_i N(y_i, k)  \frac{1}{\Phi_k} \Rightarrow \Phi_k=\frac{\sum_i N\left(y_i, k\right)}{\sum_k \sum_i N\left(y_i, k\right)}=\frac{N_k}{N} \\
\frac{\partial l}{\partial \gamma_{kn}}&=\sum_i N(x_i, k) \frac{1}{\gamma_{k n}} \Rightarrow \gamma_{k n}=\frac{\sum_i N\left(x_i, k\right)}{\sum_k \sum_i N\left(x_i, k\right)}=\frac{M_k}{M}
\end{aligned}
$$

## 隐马尔可夫模型

隐马尔可夫模型是对含有未知参数（隐状态）的马尔可夫链进行建模的生成模型。

![image-20230625091616229](assets/image-20230625091616229-7655777.png)

HMM 中有两条非常重要的假设，齐次马尔可夫假设和观测独立假设：

- 齐次一阶 Markov 假设：一阶指 $P\left(y_t \mid y_{1: t-1}\right)=P\left(y_t \mid y_{t-1}\right)$；齐次指 $P\left(y_t \mid y_{t-1}\right)$ 与时间无关，它们都是同一分布
- 观测独立假设：$P\left(x_t \mid y_{1: t}, x_{1: t-1}\right)=P\left(x_t \mid y_t\right)$

实际上这两种假设都是非必须的，都是为了简化计算，有它们反而破坏了原始数据分布的完整性。HMM模型建模如下：

$$
P(X, Y \mid \lambda) = \prod_{t=1}^T P\left(y_t \mid y_{t-1}, \lambda\right) P\left(x_t \mid y_t, \lambda\right)
$$

## 最大熵马尔可夫模型

隐马尔可夫模型是一种对隐状态序列和观测状态序列的联合概率 $P(x,y)$ 进行建模的生成式模型，而最大熵马尔可夫模型是直接对标注的后验概率 $P(y \mid x)$ 进行建模的判别式模型。

![image-20230625095408854](assets/image-20230625095408854-7658050.png)

最大熵马尔可夫模型建模如下：

$$
P(Y \mid X, \lambda)=\prod_{t=1}^T P\left(y_t \mid x_t, y_{t-1}, \lambda\right)=\prod_{t=1}^T P\left(y_t \mid x_{1: T}, y_{t-1}, \lambda\right)
$$

MEMM 比 HMM 有优势的地方：

1. 将生成模型变成了判别模型。这里好的原因是， 对于链式结构，我们主要处理的是标注问题，只要对条件建模就行了，没必要算那么多
2. 打破了观测独立假设，可以尽可能多的利用到原始数据分布之间的信息

由于局部归一化的影响，隐状态会倾向于转移到那些后续状态可能更少的状态上，以提高整体的后验概率。因此最大熵马尔可夫模型存在标注偏置问题。

假设我们将这个局部看成是一个系统的话，我可以将 $y$ 看成是系统内部的状态，$y_{t−1} → y_t$ 的转移符合一个概率分布，而 $x_t$ 被我们看成是外界的输入。我们把这个局部产生的效果记为“mass score”，可以看成关于 $y_{t−1}$、$y_t$ 和 $x_t$ 都是有关系的函数，就是系统从 t − 1 时刻到 t 时刻转移的一个得分，这个得分就是一个概率分布。而我们的问题出在哪呢？因为这三个小单体形成的一个局部，实际上是一个概率分布，概率分布就一定要归一化。如果把这个链条看成是一根长的绳子，我们在 t 这个点抖动一下这根绳子，肯定会对后续的绳子 t + 1, t + 2, · · · 都造成影响。但是如果在中间进行归一化，就相当于在某个点把绳子的结构粗细发生了变化。那么这个波就无法正常的影响到后面的绳子了，破坏了这个波能量的传递连贯性。

标注偏置问题只存在于最大熵马尔可夫模型（MEMM）中，状态之间的假设则是标注偏置问题产生的根源。在MEMM中，状态之间的转移概率依赖观测值 $x$，$x$ 不一样时，转移概率也不一样；在HMM中，我们只考虑状态和状态之间的转移，不依赖观测值 $x$，也就是对每个时刻 t ，他们的转移概率是一样的。

## 条件随机场

在 MEMM 中，它的概率图模型中带来了一个重大的缺陷就是 Label Bias Problem。打破的方法就是将各个 $y$ 节点之间的连接从有向图变成无向图。无向图局部就不是一个分布了，就不需要进行局部归一化，而是进行全局归一化了，整条链是一个分布。CRF在最大熵马尔可夫模型的基础上，进行了全局归一化，也就是让真实序列的概率最大化。

![image-20230625102204437](assets/image-20230625102204437-7659726.png)

条件随机场建模如下：

$$
P(y \mid x) = \frac{1}{Z} \prod_{i=1}^k \exp \left\{F_i\left(x_{c_i}\right)\right\}=\frac{1}{Z} \exp \sum_{i=1}^{T-1} F_t\left(y_{t-1}, y_t, x_{1: T}\right)
$$

由于线性链中所有团的结构都是一样的，我们只考虑其中的一个团结构。我们将 $F_t\left(y_{t-1}, y_t, x_{1: T}\right)$ 分成两部分：状态函数和转移函数。状态函数包括 $y_t$ 时刻和 $y_{t−1}$ 时刻和 $x_{1:T}$ 之间的影响，状态函数中 $y_t$ 对 t 时刻的影响已经包含了 $y_{t−1}$ 时刻的影响，我们没有必要再计算两遍，所以在状态函数中我们可以忽略掉 $y_{t−1}$；移函数包括 $y_t$，$y_{t−1} 时刻共同和 x 1:T之间的影响。

$$
\begin{aligned}
F\left(y_{t-1}, y_t, x_{1: T}\right) &= \triangle_{y_{t-1}, x_{1: T}}+\triangle_{y_t, x_{1: T}}+\triangle_{y_{t-1}, y_t, x_{1: T}} \\
&= \triangle_{y_t, x_{1: T}}+\triangle_{y_{t-1}, y_t, x_{1: T}} \\
\triangle_{y_t, x_{1: T}}&=\sum_{l=1}^L \eta_l g_l\left(y_t, x_{1: T}\right) \\
\triangle_{y_{t-1}, y_t, x_{1: T}}&=\sum_{k=1}^K \lambda_k f_k\left(y_{t-1}, y_t, x_{1: T}\right)
\end{aligned}
$$

$f_k$ 和 $g_l$ 都是给定的特征函数，$\lambda_k$ 和 $\eta_l$ 都是需要学习的参数。所以，我们最后就得到了 CRF 条件概率的表达形式为：

$$
P(Y \mid X)=\frac{1}{Z} \exp \left\{\sum_{t=1}^T\left[\sum_{k=1}^K \lambda_k f_k\left(y_{t-1}, y_t, x_{1: T}\right)+\sum_{l=1}^L \eta_l g_l\left(y_t, x_{1: T}\right)\right]\right\}
$$


$f(y_{t-1}, y_t, x)$ 可以理解为根据 $y_{t-1}$、$y_t$ 和 $x$ 提取的特征的加权和，考虑到当前深度学习模型中，Bert等模型已经能够使得状态函数充分地捕捉各个 $y$ 与输出 $x$ 的联系。因此，不妨考虑转移特征函数与 $x$ 无关，那么可以进一步简化为：$f(y_{t-1}, y_t, x)=f(y_{t-1}, y_t)$

LSTM可以学习到上下文的信息，但是不能限制前后两个label的关系，可能会出现前后两个B的情况。LSTM后加交叉熵损失函数，每个时刻做一个softmax归一化，和最大熵模型一样，每个时刻做局部归一化，其后果就是标注偏置。所以采用条件随机场损失，不涉及到每一个时刻概率最大化。

# 深度神经网络

1. 神经网络训练时是否可以将全部参数初始化为0？

> 全连接的深度神经网络，同一层中的任意神经元都是同构的，它们拥有相同，如果再将参数全部初始化为同样的值，那么无论前向传播还是反向传播的取值都是完全相同的。学习过程将永远无法打破这种对称性，最终同一网络层中的各个参数仍然是相同的。因此，我们需要随机地初始化神经网络参数的值，以打破这种对称性。

## Dropout

Dropout在网络的训练中，以一定的概率随机地 “临时丢弃”一部分神 经元节点。由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络。类比于Bagging方 法，Dropout可被认为是一种实用的大规模深度神经网络的模型集成算法。

Dropout的具体实现中，要求某个神经元节点激活值以一定的概率 $p$ 被“丢弃”， 即该神经元暂时停止工作。因此，对于包含N个神经元节点的网络，在Dropout的作用下可看作为 $2^N$ 个模型的集成，可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数。

对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合的风险，增强泛化能力。

预测阶段：

- 在训练的时，Dropout 没有对 $y$ 进行缩放，也就是乘以 $1 /(1-p)$。那么在预测时，就需要对权重进行缩放，即每一个神经单元的权重参数要乘以概率 $p$
- 在训练的时，Dropout 对 $y$ 进行缩放，也就是乘以 $1 /(1-p)$。那么在预测时就不用再对权重参数进行缩放

## RNN

<img src="assets/The-standard-RNN-and-unfolded-RNN.png" alt="The standard RNN and unfolded RNN. " style="zoom:50%;" />

$$
\begin{aligned}
h_t &= \sigma\left(a_t\right) \\
&= \sigma\left(U x_t+W h_{t-1}\right) \\
dh_t &= \sigma^{\prime}\left(a_t\right) \odot\left(W h_{t-1}\right) \\
dl &= \operatorname{tr}\left(\frac{\partial l}{\partial h_t^T} d h_t\right) \\
& =\operatorname{tr}\left(\frac{\partial l}{\partial h_t^T}\left[\sigma^{\prime}(a_t) \odot(W dh_{t-1})\right]\right) \\
& =\operatorname{tr}\left(\left[\frac{\partial l}{\partial h_t} \odot \sigma^{\prime}\left(a_t\right)\right]^T W d h_{t-1}\right) \\
\frac{\partial l}{\partial h_{t-1}} & =W^T \left[\frac{\partial l}{\partial h_t} \odot \sigma^{\prime}(a_t)\right]
\end{aligned}
$$

当采用ReLU作为循环神经网络中隐含层的激活函数时，只有当 $W$ 的取值在单位矩阵附近时才能取得比较好的效果。

## LSTM

<img src="assets/1*o9R4WZZulh7-vtfUPQsEjQ.png" alt="img" style="zoom:50%;" />

$$
\begin{aligned}
f_t & =\sigma\left(W_f x_t+U_f h_{t-1}+b_f\right) \\
i_t & =\sigma\left(W_i x_t+U_i h_{t-1}+b_i\right) \\
o_t & =\sigma\left(W_o x_t+U_o h_{t-1}+b_o\right) \\
\hat{c}_t & =\tanh \left(W_c x_t+U_c h_{t-1}+b_c\right) \\
c_t & =f_t \odot c_{t-1}+i_t \odot \hat{c}_t \\
h_t & =o_t \odot \tanh \left(c_t\right)
\end{aligned}
$$

RNN 中的梯度消失/梯度爆炸和普通的 MLP 或者深层 CNN 中梯度消失/梯度爆炸的含义不一样。MLP/CNN 中不同的层有不同的参数，各是各的梯度；而 RNN 中同样的权重在各个时间步共享，最终的梯度 g = 各个时间步的梯度 g_t 的和。RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。


LSTM 中梯度的传播有很多条路径：$c_{t-1} \rightarrow c_t=f_t \odot c_{t-1}+i_t \odot \hat{c_t}$ 这条路径上只有逐元素相乘和相加的操作$\frac{\partial l}{\partial c_{t-1}} = \frac{\partial l}{\partial c_t} \odot f_t$，梯度流最稳定；但是其他路径 (例如 $c_{t-1} \rightarrow h_{t-1} \rightarrow i_t \rightarrow c_t$ ) 上梯度流与普通 RNN 类似，照样会发生相同的权重矩阵反复连乘。

LSTM 刚提出时没有遗忘门, 或者说相当于 $f_t=1$，这时候在 $c_{t-1} \rightarrow c_t$ 直接相连的短路路径上，$d l / d c_t$ 可以无损地传递给 $d l / d c_{t-1}$，从而这条路径上的梯度畅通无阻，不会消失。类似 于 ResNet 中的残差连接。

但是在其他路径上，LSTM 的梯度流和普通 RNN 没有太大区别，依然会爆炸或者消失。由于总的远距离梯度 = 各条路径的远距离梯度之和，即便其他远距离路径梯度消失了，只要保证有一条远距离路径 $c_{t-1} \rightarrow c_t$ 梯度不消失，总的远距离梯度就不会消失。因此 LSTM 通过改善一条路径上的梯度问题拯救了总体的远距离梯度。

同样，因为总的远距离梯度 = 各条路径的远距离梯度之和，路径 $c_{t-1} \rightarrow c_t$ 梯度流比较稳定，但其他路径上梯度有可能爆炸，此时总的远距离梯度 = 正常梯度 + 爆炸梯度 = 爆炸梯度，因此 LSTM 仍然有可能发生梯度爆炸。不过，由于 LSTM 的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于 1），因此 LSTM 发生梯度爆炸的频率要低得多。实践中梯度爆炸一般通过梯度裁剪来解决。

一个重要的地方是 $f_t, i_t, o_t, \tilde{c}_t$ 都是神经网络自己学习到的。所以说，神经网络会通过学习改变门控的值来决定什么时候遗忘梯度，什么时候保留梯度。有个相当自洽的结论：如果我们的任务比较依赖于历史信息，那么 $f_t$ 就会接近于1，这时候历史的梯度信息也正好不容易消失；如果 $f_t$ 很接近于0，那么就说明我们的任务不依赖于历史信息，这时候就算梯度消失也无妨了。当然，常常存在 f 介于 [0, 1] 之间的情况，在这种情况下只能说 LSTM 改善（而非解决）了梯度消失的状况。
